import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(page_title="ë°ì´í„° íŒ¨í„´ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ", layout="wide")

if 'draw_graph' not in st.session_state:
    st.session_state['draw_graph'] = False

@st.cache_data
def load_csv_files_from_folder(folder_path):
    """í´ë”ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ"""
    csv_files = []
    failed_files = []  # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    folder = Path(folder_path)
    
    if not folder.exists():
        return []
    with st.spinner("íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤..."):
        for file_path in folder.glob("*.csv"):
            try:
                df = pd.read_csv(file_path)
                
                # ì»¬ëŸ¼ëª… ì •ë¦¬: ì•ë’¤ ê³µë°± ì œê±°
                df.columns = df.columns.str.strip()
                
                # ë¹ˆ ì»¬ëŸ¼ëª…ì´ë‚˜ ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
                df.columns = [col if col else f'unnamed_{i}' for i, col in enumerate(df.columns)]
                
                # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
                cols = df.columns.tolist()
                seen = {}
                for i, col in enumerate(cols):
                    if col in seen:
                        seen[col] += 1
                        cols[i] = f"{col}_{seen[col]}"
                    else:
                        seen[col] = 0
                df.columns = cols
                
                csv_files.append({
                    'filename': file_path.name,
                    'filepath': str(file_path),
                    'dataframe': df,
                    'shape': df.shape
                })
            except Exception as e:
                failed_files.append(f"**{file_path.name}**: {str(e)}")
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì´ ìˆìœ¼ë©´ í•˜ë‚˜ì˜ expanderë¡œ í‘œì‹œ
    if failed_files:
        with st.expander(f"âš ï¸ ì½ê¸° ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ ({len(failed_files)}ê°œ) - í´ë¦­í•´ì„œ ìƒì„¸ ë³´ê¸°"):
            for failed_file in failed_files:
                st.error(failed_file)
    
    return csv_files

@st.cache_data
def analyze_column_relationships(dataframes):
    """ì»¬ëŸ¼ë“¤ ê°„ì˜ ê´€ê³„ì„± ë¶„ì„"""
    all_columns = set()
    column_types = {}
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì—ì„œ ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°
    for df_info in dataframes:
        df = df_info['dataframe']
        all_columns.update(df.columns)
        
        for col in df.columns:
            if col not in column_types:
                column_types[col] = []
            column_types[col].append(df[col].dtype)
    
    # ì»¬ëŸ¼ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    timestamp_cols = []
    numeric_cols = []
    categorical_cols = []
    
    for col in all_columns:
        # ì»¬ëŸ¼ëª… ì •ë¦¬ í›„ ë¶„ì„
        col_clean = str(col).strip().lower()
        
        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì‹ë³„
        if any(keyword in col_clean for keyword in ['timestamp', 'time', 'date',  'ì‹œê°„', 'ë‚ ì§œ']):
            timestamp_cols.append(col)
        # ìˆ«ìí˜• ì»¬ëŸ¼ ì‹ë³„
        elif any(pd.api.types.is_numeric_dtype(dtype) for dtype in column_types[col]):
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)
    
    return {
        'timestamp_cols': timestamp_cols,
        'numeric_cols': numeric_cols,
        'categorical_cols': categorical_cols,
        'all_columns': list(all_columns)
    }

def get_incremental_keyword_groups(columns, default_keywords, additional_keywords):
    """ì¦ë¶„ ì—…ë°ì´íŠ¸ë¡œ í‚¤ì›Œë“œ ê·¸ë£¹í™” - ë³€ê²½ëœ ë¶€ë¶„ë§Œ ë‹¤ì‹œ ê³„ì‚°"""
    
    # Session State ì´ˆê¸°í™”
    if 'keyword_groups_cache' not in st.session_state:
        st.session_state.keyword_groups_cache = {}
    if 'prev_default_keywords' not in st.session_state:
        st.session_state.prev_default_keywords = set()
    if 'prev_additional_keywords' not in st.session_state:
        st.session_state.prev_additional_keywords = set()
    
    # í˜„ì¬ í‚¤ì›Œë“œ ì„¸íŠ¸
    current_default = set(default_keywords)
    current_additional = set(additional_keywords)
    
    # ì´ì „ í‚¤ì›Œë“œ ì„¸íŠ¸
    prev_default = st.session_state.prev_default_keywords
    prev_additional = st.session_state.prev_additional_keywords
    
    # ìºì‹œëœ ê·¸ë£¹ë“¤
    cached_groups = st.session_state.keyword_groups_cache.copy()
    
    # 1. ì‚­ì œëœ í‚¤ì›Œë“œë“¤ ì œê±°
    deleted_default = prev_default - current_default
    deleted_additional = prev_additional - current_additional
    
    for keyword in deleted_default | deleted_additional:
        if keyword in cached_groups:
            del cached_groups[keyword]
            st.info(f"ğŸ—‘ï¸ '{keyword}' ê·¸ë£¹ ì‚­ì œë¨")
    
    # 2. ìƒˆë¡œ ì¶”ê°€ëœ í‚¤ì›Œë“œë“¤ë§Œ ê³„ì‚°
    new_default = current_default - prev_default
    new_additional = current_additional - prev_additional
    
    # ìƒˆë¡œìš´ ê¸°ë³¸ í‚¤ì›Œë“œë“¤ - ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
    for keyword in new_default:
        keyword_lower = keyword.lower().strip()
        matching_cols = []
        
        for col in columns:
            col_clean = str(col).strip().lower()
            if keyword_lower in col_clean:
                matching_cols.append(col)
        
        if matching_cols:
            cached_groups[keyword] = matching_cols
            #st.success(f"âœ… '{keyword}' ê·¸ë£¹ ì¶”ê°€ë¨: {len(matching_cols)}ê°œ ì»¬ëŸ¼")
    
    # ìƒˆë¡œìš´ ì¶”ê°€ í‚¤ì›Œë“œë“¤ - ì •í™•í•œ ë§¤ì¹­
    for keyword in new_additional:
        keyword_lower = keyword.lower().strip()
        matching_cols = []
        
        for col in columns:
            col_clean = str(col).strip().lower()
            if keyword_lower == col_clean:
                matching_cols.append(col)
        
        if matching_cols:
            cached_groups[keyword] = matching_cols
            #st.success(f"ğŸ¯ '{keyword}' ê·¸ë£¹ ì¶”ê°€ë¨: {len(matching_cols)}ê°œ ì»¬ëŸ¼")
    
    # 3. Session State ì—…ë°ì´íŠ¸
    st.session_state.keyword_groups_cache = cached_groups
    st.session_state.prev_default_keywords = current_default
    st.session_state.prev_additional_keywords = current_additional
    
    # 4. ë³€ê²½ ì‚¬í•­ ìš”ì•½ í‘œì‹œ
    if new_default or new_additional or deleted_default or deleted_additional:
        with st.expander("ğŸ”„ í‚¤ì›Œë“œ ë³€ê²½ ì‚¬í•­"):
            if new_default:
                st.write(f"**ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ë³¸ í‚¤ì›Œë“œ:** {', '.join(new_default)}")
            if new_additional:
                st.write(f"**ìƒˆë¡œ ì¶”ê°€ëœ ì •í™• í‚¤ì›Œë“œ:** {', '.join(new_additional)}")
            if deleted_default or deleted_additional:
                deleted_all = deleted_default | deleted_additional
                st.write(f"**ì‚­ì œëœ í‚¤ì›Œë“œ:** {', '.join(deleted_all)}")
    
    return cached_groups

def reset_keyword_cache():
    """í‚¤ì›Œë“œ ìºì‹œ ì´ˆê¸°í™” í•¨ìˆ˜"""
    if 'keyword_groups_cache' in st.session_state:
        del st.session_state.keyword_groups_cache
    if 'prev_default_keywords' in st.session_state:
        del st.session_state.prev_default_keywords
    if 'prev_additional_keywords' in st.session_state:
        del st.session_state.prev_additional_keywords
    st.success("ğŸ”„ í‚¤ì›Œë“œ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")


def create_keyword_aggregated_dataframe(dataframes, x_col, keyword_groups, agg_method='mean', time_agg_method='ì •í™•í•œ ì‹œê°„'):
    """í‚¤ì›Œë“œ ê·¸ë£¹ë³„ë¡œ ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    combined_data = []
    aggregation_summary = []  # ì§‘ê³„ ìš”ì•½ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    
    for df_info in dataframes:
        df = df_info['dataframe'].copy()
        df['source_file'] = df_info['filename']
        
        # Xì¶• ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ê³µë°± ì œê±° í›„)
        x_col_clean = str(x_col).strip()
        if x_col_clean not in df.columns:
            continue
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ë¥¼ ë¨¼ì € ìˆ˜í–‰
        col_clean = str(x_col).strip().lower()
        if any(keyword in col_clean for keyword in ['time', 'date', 'timestamp']):
            try:
                df[x_col_clean] = pd.to_datetime(df[x_col_clean])
                
                # ì‹œê°„ ì§‘ê³„ ë°©ë²•ì— ë”°ë¼ ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
                if time_agg_method == "ì‹œê°„ë³„":
                    df['time_group'] = df[x_col_clean].dt.floor('H')
                elif time_agg_method == "ì¼ë³„":
                    df['time_group'] = df[x_col_clean].dt.date
                elif time_agg_method == "ì›”ë³„":
                    df['time_group'] = df[x_col_clean].dt.to_period('M')
                else:  # ì •í™•í•œ ì‹œê°„
                    df['time_group'] = df[x_col_clean]
                    
            except Exception as e:
                st.warning(f"ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
                df['time_group'] = df[x_col_clean]
        else:
            df['time_group'] = df[x_col_clean]
            
        df_subset = df[['time_group', 'source_file']].copy()
        
        # ê° í‚¤ì›Œë“œ ê·¸ë£¹ë³„ë¡œ í‰ê·  ê³„ì‚°
        for keyword, cols in keyword_groups.items():
            # ì»¬ëŸ¼ëª… ì •ë¦¬ í›„ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì°¾ê¸°
            available_cols = []
            for col in cols:
                col_clean = str(col).strip()
                if col_clean in df.columns and pd.api.types.is_numeric_dtype(df[col_clean]):
                    available_cols.append(col_clean)
            
            if available_cols:
                try:
                    if agg_method == 'mean':
                        df_subset[f'{keyword}_í‰ê· '] = df[available_cols].mean(axis=1)
                    elif agg_method == 'sum':
                        df_subset[f'{keyword}_í•©ê³„'] = df[available_cols].sum(axis=1)
                    elif agg_method == 'median':
                        df_subset[f'{keyword}_ì¤‘ì•™ê°’'] = df[available_cols].median(axis=1)
                    
                    # ì§‘ê³„ ìš”ì•½ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (ê°œë³„ ì¶œë ¥ ëŒ€ì‹ )
                    aggregation_summary.append({
                        'keyword': keyword,
                        'status': 'success',
                        'count': len(available_cols),
                        'columns': available_cols
                    })
                        
                except Exception as e:
                    aggregation_summary.append({
                        'keyword': keyword,
                        'status': 'error',
                        'error': str(e)
                    })
        
        combined_data.append(df_subset)
    
    # ì§‘ê³„ ìš”ì•½ì„ í•˜ë‚˜ì˜ expanderë¡œ í‘œì‹œ
    if aggregation_summary:
        with st.expander(f"ğŸ“Š í‚¤ì›Œë“œ ê·¸ë£¹ ì§‘ê³„ ê²°ê³¼ ({len(aggregation_summary)}ê°œ ê·¸ë£¹)"):
            for summary in aggregation_summary:
                if summary['status'] == 'success':
                    st.write(f"âœ… **'{summary['keyword']}' ê·¸ë£¹**: {summary['count']}ê°œ ì»¬ëŸ¼ ì§‘ê³„ë¨")
                    st.write(f"&nbsp;&nbsp;&nbsp;&nbsp;ì‚¬ìš©ëœ ì»¬ëŸ¼: {', '.join(summary['columns'])}")
                else:
                    st.write(f"âš ï¸ **'{summary['keyword']}' ê·¸ë£¹**: ì§‘ê³„ ì¤‘ ì˜¤ë¥˜ - {summary['error']}")
                st.write("---")
    
    if not combined_data:
        return None
    
    # ë°ì´í„° í†µí•©
    combined_df = pd.concat(combined_data, ignore_index=True)
    
    # time_group ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„
    value_cols = [col for col in combined_df.columns if col not in ['time_group', 'source_file']]
    
    if not value_cols:
        st.error("ì§‘ê³„í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        if agg_method == 'mean':
            agg_df = combined_df.groupby('time_group')[value_cols].mean().reset_index()
        elif agg_method == 'sum':
            agg_df = combined_df.groupby('time_group')[value_cols].sum().reset_index()
        elif agg_method == 'median':
            agg_df = combined_df.groupby('time_group')[value_cols].median().reset_index()
        else:
            agg_df = combined_df.groupby('time_group')[value_cols].mean().reset_index()
        
        # ì›ë˜ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
        agg_df = agg_df.rename(columns={'time_group': x_col_clean})
        
        # ì§‘ê³„ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        st.info(f"ğŸ“Š ì§‘ê³„ ì™„ë£Œ: {len(agg_df)}ê°œì˜ ì‹œê°„ êµ¬ê°„, {len(value_cols)}ê°œì˜ í‚¤ì›Œë“œ ê·¸ë£¹")
        
        # ì‹œê°„ ë²”ìœ„ í‘œì‹œ
        if len(agg_df) > 0:
            time_range_start = agg_df[x_col_clean].min()
            time_range_end = agg_df[x_col_clean].max()
            st.write(f"â° ë¶„ì„ ê¸°ê°„: {time_range_start} ~ {time_range_end}")
        
        return agg_df
    except Exception as e:
        st.error(f"ë°ì´í„° ì§‘ê³„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.write(f"ë””ë²„ê·¸ ì •ë³´: ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ = {value_cols}")
        return None

def create_aggregated_dataframe(dataframes, group_cols, value_cols, agg_method='mean'):
    """ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆì„ì„ í†µí•©í•˜ì—¬ ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    combined_data = []
    
    for df_info in dataframes:
        df = df_info['dataframe'].copy()
        df['source_file'] = df_info['filename']
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_cols = [col for col in group_cols + value_cols if col in df.columns]
        if len(available_cols) < 2:
            continue
            
        df_subset = df[available_cols + ['source_file']]
        combined_data.append(df_subset)
    
    if not combined_data:
        return None
    
    # ë°ì´í„° í†µí•©
    combined_df = pd.concat(combined_data, ignore_index=True)
    
    # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
    for col in group_cols:
        if col in combined_df.columns:
            col_lower = col.lower().strip()
            if any(keyword in col_lower for keyword in ['timestamp', 'time', 'date', ]):
                try:
                    combined_df[col] = pd.to_datetime(combined_df[col])
                except:
                    pass
    
    # ì§‘ê³„ ìˆ˜í–‰
    if agg_method == 'mean':
        agg_df = combined_df.groupby(group_cols)[value_cols].mean().reset_index()
    elif agg_method == 'sum':
        agg_df = combined_df.groupby(group_cols)[value_cols].sum().reset_index()
    elif agg_method == 'median':
        agg_df = combined_df.groupby(group_cols)[value_cols].median().reset_index()
    else:
        agg_df = combined_df.groupby(group_cols)[value_cols].mean().reset_index()
    
    return agg_df

def create_pattern_analysis_chart(df, x_col, y_cols, chart_type="Line"):
    """íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ì°¨íŠ¸ ìƒì„±"""
    if chart_type == "Multi-Line with Correlation":
        # ì„œë¸Œí”Œë¡¯ ìƒì„± (ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ í¬í•¨)
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=('ì‹œê³„ì—´ íŒ¨í„´', 'ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', 'ë¶„í¬ ë¶„ì„', 'ì •ê·œí™” ë¹„êµ'),
            specs=[[{"colspan": 2}, None],
                    [{"type": "xy"}, {"type": "xy"}],
                    [{"colspan": 2}, None]
                    ]
        )
        
        # 1. ì‹œê³„ì—´ íŒ¨í„´
        for y_col in y_cols:
            if y_col in df.columns:
                fig.add_trace(
                    go.Scatter(x=df[x_col], y=df[y_col], name=y_col, mode='lines+markers'),
                    row=1, col=1
                )
        
        # 2. ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ)
        numeric_cols = [col for col in y_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            
            fig.add_trace(
                go.Heatmap(
                    z=corr_matrix.values,
                    x=corr_matrix.columns,
                    y=corr_matrix.columns,
                    colorscale='RdBu',
                    zmid=0,
                    colorbar=dict(
                        x=0.47,  # ì™¼ìª½ìœ¼ë¡œ ì´ë™ (0~1 ë²”ìœ„)
                        y=0.50,  # ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™ (0~1 ë²”ìœ„)
                        len=0.25,  # ì»¬ëŸ¬ë°” ê¸¸ì´ ì¡°ì •
                        thickness=15,  # ì»¬ëŸ¬ë°” ë‘ê»˜ ì¡°ì •
                        title=dict(
                            text="ìƒê´€ê³„ìˆ˜",
                            side="right"
                        )
                    )
                ),
                row=2, col=1
            )
        
        # 3. ë¶„í¬ ë¶„ì„ (ë°•ìŠ¤í”Œë¡¯)
        for i, y_col in enumerate(numeric_cols[:3]):  # ìµœëŒ€ 3ê°œë§Œ
            fig.add_trace(
                go.Box(y=df[y_col], name=y_col),
                row=2, col=2
            )
            
        # 4.ì •ê·œí™” ë¶„ì„
        for y_col in y_cols:
            if pd.api.types.is_numeric_dtype(aggregated_df[y_col]):
                col_data = aggregated_df[y_col]
                normalized = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                fig.add_trace(
                    go.Scatter(x=aggregated_df[x_col], y=normalized, 
                                name=f"{y_col} (ì •ê·œí™”)", mode='lines'),
                    row=3, col=1
                )    
        
        fig.update_layout(height=1000, title_text="ì¢…í•© íŒ¨í„´ ë¶„ì„", margin=dict(r=120))
        return fig
    
    elif chart_type == "Normalized Comparison":
        # ì •ê·œí™”ëœ ë¹„êµ ì°¨íŠ¸
        fig = go.Figure()
        
        # ê° ì»¬ëŸ¼ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        for y_col in y_cols:
            if y_col in df.columns and pd.api.types.is_numeric_dtype(df[y_col]):
                normalized_values = (df[y_col] - df[y_col].min()) / (df[y_col].max() - df[y_col].min())
                fig.add_trace(
                    go.Scatter(
                        x=df[x_col], 
                        y=normalized_values, 
                        name=f"{y_col} (ì •ê·œí™”)", 
                        mode='lines+markers'
                    )
                )
        
        fig.update_layout(
            title="ì •ê·œí™”ëœ ë°ì´í„° ë¹„êµ (0-1 ìŠ¤ì¼€ì¼)",
            yaxis_title="ì •ê·œí™”ëœ ê°’",
            height=600
        )
        return fig
    
    else:
        # ê¸°ë³¸ ì°¨íŠ¸ë“¤
        if chart_type == "Line":
            fig = px.line(df, x=x_col, y=y_cols, title="ë¼ì¸ ì°¨íŠ¸ - íŒ¨í„´ ë¶„ì„")
        elif chart_type == "Scatter":
            fig = px.scatter(df, x=x_col, y=y_cols, title="ì‚°ì ë„ - íŒ¨í„´ ë¶„ì„")
        elif chart_type == "Bar":
            fig = px.bar(df, x=x_col, y=y_cols, title="ë§‰ëŒ€ ì°¨íŠ¸ - íŒ¨í„´ ë¶„ì„")
        
        return fig

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.subheader("Server Info")
    st.markdown("**ì‹œí¥ gpuserver2** (59.14.241.229) - 5090*3")
        
    st.title("âš™ï¸ ì„¤ì •")   

    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
    data_source = st.radio("ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ", ["ë‹¨ì¼ íŒŒì¼", "í´ë” ë¶„ì„"])
    
    if data_source == "ë‹¨ì¼ íŒŒì¼":
        uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
        use_sample_data = st.checkbox("ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
    else:
        st.write("íŒŒì¼ ê²½ë¡œ ì…ë ¥ì „ í‚¤ì›Œë“œ ì„¤ì •ì„ ë¯¸ë¦¬í•˜ëŠ”ê±¸ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.")
        folder_path = st.text_input("í´ë” ê²½ë¡œ ì…ë ¥", value="./sample")
        
    # ë¶„ì„ ëª¨ë“œ ì„ íƒ
    analysis_mode = st.radio("ë¶„ì„ ëª¨ë“œ", ["í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„", "ìˆ˜ë™ ì„ íƒ"]) # "ìë™ íŒ¨í„´ ë¶„ì„" ì¼ë‹¨ ì œê±°í•¨
        
    if analysis_mode == "í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„":
        st.subheader("ğŸ” í‚¤ì›Œë“œ ì„¤ì •")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì œì•ˆ
        default_keywords = st.text_input(
            "í¬í•¨ ê²€ìƒ‰ í‚¤ì›Œë“œ(ì‰¼í‘œë¡œ êµ¬ë¶„)", 
            value="cell, temperature, current",
            help="ì…ë ¥í•œ ë‹¨ì–´ê°€ í¬í•¨ëœ ëª¨ë“  ì»¬ëŸ¼ì„ ì°¾ìŠµë‹ˆë‹¤. ì˜ˆ: 'cell' â†’ cell30, cell31, cell32 ë“±"
        )
        keywords = [k.strip() for k in default_keywords.split(",") if k.strip()]
        
        # ì¶”ê°€ í‚¤ì›Œë“œ
        additional_keywords = st.text_area(
            "ì •í™• ì¼ì¹˜ í‚¤ì›Œë“œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
            placeholder="ì˜ˆ:\speed\mileage\soh",
            help="ì…ë ¥í•œ ë‹¨ì–´ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì°¾ìŠµë‹ˆë‹¤. ì˜ˆ: 'soc' â†’ soc ì»¬ëŸ¼ë§Œ"
        )
        if additional_keywords:
            keywords.extend([k.strip() for k in additional_keywords.split("\n") if k.strip()])
    
    agg_method = st.selectbox("ì§‘ê³„ ë°©ë²•", ["mean", "sum", "median"])
    
    chart_type = st.selectbox("ì°¨íŠ¸ íƒ€ì… ì„ íƒ", [
        "Line", "Scatter", "Bar", 
        "Multi-Line with Correlation", 
        "Normalized Comparison"
    ])
    show_table = st.checkbox("ì „ì²´ ë°ì´í„° í…Œì´ë¸” ë³´ê¸°")

# ë©”ì¸ í™”ë©´
st.title("ğŸ“Š ë°ì´í„° ì‹œê°í™” ëŒ€ì‹œë³´ë“œ")
st.write("ì—…ë¡œë“œí•œ ë°ì´í„° ë˜ëŠ” í´ë” ë‚´ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# í´ë” ë¶„ì„ ëª¨ë“œ
if data_source == "í´ë” ë¶„ì„":
    if folder_path:
        csv_files = load_csv_files_from_folder(folder_path)
else:
    csv_files = []
    df = pd.DataFrame()

    try:
        # ë°ì´í„° ì½ê¸°
        if use_sample_data:
            filepath = "sample/628dani_V031BL0000_CASPER LONGRANGE_202410.csv"
            df = pd.read_csv(filepath)
            filename = "sampledata.csv"
        elif uploaded_file:
            df = pd.read_csv(uploaded_file)
            filepath = str(uploaded_file)
            filename = uploaded_file.name
        else:
            raise ValueError("íŒŒì¼ ì—†ìŒ")

        # ì»¬ëŸ¼ëª… ì •ë¦¬
        df.columns = df.columns.str.strip()  # ì•ë’¤ ê³µë°± ì œê±°
        df.columns = [col if col else f'unnamed_{i}' for i, col in enumerate(df.columns)]  # ë¹ˆ ì»¬ëŸ¼ ì²˜ë¦¬

        # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
        seen = {}
        new_cols = []
        for col in df.columns:
            if col in seen:
                seen[col] += 1
                new_cols.append(f"{col}_{seen[col]}")
            else:
                seen[col] = 0
                new_cols.append(col)
        df.columns = new_cols

        # ê²°ê³¼ ì¶”ê°€
        csv_files.append({
            'filename': filename,
            'filepath': filepath,
            'dataframe': df,
            'shape': df.shape
        })

    except Exception as e:
        st.warning("âš ï¸ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        

if csv_files:
    st.success(f"âœ… {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
    with st.expander("ë°œê²¬ëœ íŒŒì¼ë“¤"):
        for file_info in csv_files:
            st.write(f"- **{file_info['filename']}**: {file_info['shape']} (í–‰, ì—´)")
    
    # ì»¬ëŸ¼ ê´€ê³„ ë¶„ì„
    column_analysis = analyze_column_relationships(csv_files)
    
    st.subheader("ğŸ” ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼**")
        for col in column_analysis['timestamp_cols']:
            st.write(f"- {col}")
    
    with col2:
        st.write("**ìˆ«ìí˜• ì»¬ëŸ¼**")
        for col in column_analysis['numeric_cols'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            st.write(f"- {col}")
        if len(column_analysis['numeric_cols']) > 10:
            st.write(f"... ì™¸ {len(column_analysis['numeric_cols']) - 10}ê°œ")
    
    with col3:
        st.write("**ë²”ì£¼í˜• ì»¬ëŸ¼**")
        for col in column_analysis['categorical_cols'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            st.write(f"- {col}")
        if len(column_analysis['categorical_cols']) > 10:
            st.write(f"... ì™¸ {len(column_analysis['categorical_cols']) - 10}ê°œ")
    
    # ë¶„ì„ ëª¨ë“œë³„ ì²˜ë¦¬
    if analysis_mode == "í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„":
        
        default_keywords_list = [k.strip() for k in default_keywords.split(",") if k.strip()]
        additional_keywords_list = [k.strip() for k in additional_keywords.split("\n") if k.strip()] if additional_keywords else []
    
        # í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„
        if (default_keywords_list or additional_keywords_list) and column_analysis['timestamp_cols']:
            
            st.divider()
            st.subheader("ğŸ•’ ì‹œê°„ ì§‘ê³„ ì„¤ì •")
            
            default_index = column_analysis['timestamp_cols'].index('timestamp') if 'timestamp' in column_analysis['timestamp_cols'] else 0
            
            x_col = st.selectbox("ì‹œê°„ì¶• ì»¬ëŸ¼", column_analysis['timestamp_cols'], index=default_index)
            
            # ì‹œê°„ ì§‘ê³„ ë°©ë²• ì„ íƒ ì¶”ê°€
            time_agg_method = st.selectbox(
                "ì‹œê°„ ì§‘ê³„ ë°©ë²•", 
                ["ì •í™•í•œ ì‹œê°„", "ì‹œê°„ë³„", "ì¼ë³„", "ì›”ë³„"],
                help="ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ëŒ€ì˜ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì§‘ê³„í• ì§€ ì„ íƒí•˜ì„¸ìš”"
            )
            
            if time_agg_method != "ì •í™•í•œ ì‹œê°„":
                st.info(f"ğŸ’¡ {time_agg_method} ì§‘ê³„: ê°™ì€ {time_agg_method.replace('ë³„', '')} ë‚´ì˜ ë°ì´í„°ë“¤ì„ í‰ê· í™”í•©ë‹ˆë‹¤")
            st.divider()
            
            st.subheader("ğŸ“Š í‚¤ì›Œë“œë³„ ì»¬ëŸ¼ ê·¸ë£¹")
            # í‚¤ì›Œë“œë³„ ì»¬ëŸ¼ ê·¸ë£¹í™”
            keyword_groups = get_incremental_keyword_groups(
                column_analysis['numeric_cols'], 
                default_keywords_list, 
                additional_keywords_list
            )
            
            
            if keyword_groups:
                # ê° í‚¤ì›Œë“œ ê·¸ë£¹ í‘œì‹œ
                for keyword, cols in keyword_groups.items():
                    with st.expander(f"ğŸ”˜ '{keyword}' ê´€ë ¨ ì»¬ëŸ¼ë“¤ ({len(cols)}ê°œ)"):
                        for col in cols:
                            st.write(f"- {col}")
                        
                # if st.button("í‚¤ì›Œë“œ ê·¸ë£¹ ì´ˆê¸°í™”"):
                #     reset_keyword_cache()
                    
                # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸš€ í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„ ì‹¤í–‰", type="primary"):
                    with st.spinner("í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        aggregated_df = create_keyword_aggregated_dataframe(
                            csv_files, x_col, keyword_groups, agg_method, time_agg_method
                        )
                    
                    if aggregated_df is not None and len(aggregated_df) > 0:
                        st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ê·¸ë£¹ë³„ íŒ¨í„´ ë¶„ì„")
                        
                        # ìš”ì•½ í†µê³„
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ë¶„ì„ëœ í‚¤ì›Œë“œ ê·¸ë£¹", len(keyword_groups))
                        with col2:
                            st.metric("ë°ì´í„° í¬ì¸íŠ¸", len(aggregated_df))
                        with col3:
                            group_cols = [col for col in aggregated_df.columns if col != x_col]
                            st.metric("ìƒì„±ëœ ì§€í‘œ", len(group_cols))
                        
                        # í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì°¨íŠ¸
                        y_cols = [col for col in aggregated_df.columns if col != x_col]
                        
                        if chart_type == "Multi-Line with Correlation":
                            # ì„œë¸Œí”Œë¡¯ ìƒì„±
                            fig = make_subplots(
                                rows=2, cols=2,
                                subplot_titles=('ê·¸ë£¹ë³„ ì‹œê³„ì—´ íŒ¨í„´', 'ê·¸ë£¹ê°„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', 'ì •ê·œí™” ë¹„êµ'),
                                specs=[[{"colspan": 2}, None],
                                        [{"type": "xy"}, {"type": "xy"}]]
                            )
                            
                            # 1. ê·¸ë£¹ë³„ ì‹œê³„ì—´ íŒ¨í„´
                            for y_col in y_cols:
                                fig.add_trace(
                                    go.Scatter(x=aggregated_df[x_col], y=aggregated_df[y_col], 
                                                name=y_col, mode='lines+markers'),
                                    row=1, col=1
                                )
                            
                            # 2. ê·¸ë£¹ë³„ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
                            if len(y_cols) > 1:
                                corr_matrix = aggregated_df[y_cols].corr()
                                fig.add_trace(
                                    go.Heatmap(
                                        z=corr_matrix.values,
                                        x=corr_matrix.columns,
                                        y=corr_matrix.columns,
                                        colorscale='RdBu',
                                        zmid=0,
                                        showscale=True,
                                        colorbar=dict(
                                            x=0.47,  # ì™¼ìª½ìœ¼ë¡œ ì´ë™ (0~1 ë²”ìœ„)
                                            y=0.22,  # ì•„ë˜ìª½ìœ¼ë¡œ ì´ë™ (0~1 ë²”ìœ„)
                                            len=0.35,  # ì»¬ëŸ¬ë°” ê¸¸ì´ ì¡°ì •
                                            thickness=15,  # ì»¬ëŸ¬ë°” ë‘ê»˜ ì¡°ì •
                                            title=dict(
                                                text="ìƒê´€ê³„ìˆ˜",
                                                side="right"
                                            )
                                        )
                                    ),
                                    row=2, col=1
                                )
                            
                            # 3. ì •ê·œí™” ë¹„êµ
                            for y_col in y_cols:
                                if pd.api.types.is_numeric_dtype(aggregated_df[y_col]):
                                    col_data = aggregated_df[y_col]
                                    normalized = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                                    fig.add_trace(
                                        go.Scatter(x=aggregated_df[x_col], y=normalized, 
                                                    name=f"{y_col} (ì •ê·œí™”)", mode='lines'),
                                        row=2, col=2
                                    )
                            
                            fig.update_layout(height=800, title_text="í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì¢…í•© ë¶„ì„", margin=dict(r=120) )
                            st.plotly_chart(fig, use_container_width=True)
                        
                        else:
                            # ê¸°ë³¸ ì°¨íŠ¸
                            fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ìƒì„¸ í†µê³„ ì •ë³´
                        with st.expander("ğŸ“‹ ìƒì„¸ í†µê³„ ì •ë³´"):
                            st.write("**ê° í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ê¸°ë³¸ í†µê³„:**")
                            for col in y_cols:
                                col_stats = aggregated_df[col].describe()
                                st.write(f"**{col}**")
                                st.write(f"- í‰ê· : {col_stats['mean']:.2f}")
                                st.write(f"- í‘œì¤€í¸ì°¨: {col_stats['std']:.2f}")
                                st.write(f"- ìµœì†Ÿê°’: {col_stats['min']:.2f}")
                                st.write(f"- ìµœëŒ“ê°’: {col_stats['max']:.2f}")
                                st.write("---")
                        
                        # ë°ì´í„° í…Œì´ë¸” (ì˜µì…˜)
                        if show_table:
                            st.subheader("ì§‘ê³„ëœ í‚¤ì›Œë“œ ê·¸ë£¹ ë°ì´í„°")
                            st.dataframe(aggregated_df, use_container_width=True)
                            
                            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            csv = aggregated_df.to_csv(index=False)
                            st.download_button(
                                label="í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                                data=csv,
                                file_name=f"keyword_group_analysis_{agg_method}.csv",
                                mime="text/csv"
                            )
                    else:
                        st.warning("âš ï¸ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë‚˜ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.warning("âš ï¸ ì§€ì •ëœ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ’¡ ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  ì‹œê°„ì¶• ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            
    
    elif analysis_mode == "ìë™ íŒ¨í„´ ë¶„ì„":
        # ê¸°ì¡´ ìë™ ë¶„ì„ ë¡œì§
        if column_analysis['timestamp_cols'] and column_analysis['numeric_cols']:
            
            default_index = column_analysis['timestamp_cols'].index('timestamp') if 'timestamp' in column_analysis['timestamp_cols'] else 0
            
            x_col = column_analysis['timestamp_cols'][default_index]
            y_cols = column_analysis['numeric_cols'][:5]
            
            st.success(f"ğŸ¤– ìë™ ì„ íƒ: Xì¶•={x_col}, Yì¶•={', '.join(y_cols)}")
            
            with st.spinner("ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                aggregated_df = create_aggregated_dataframe(csv_files, [x_col], y_cols, agg_method)
            
            if aggregated_df is not None:
                st.subheader("ğŸ“ˆ íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì´ ë°ì´í„° í¬ì¸íŠ¸", len(aggregated_df))
                with col2:
                    st.metric("ë¶„ì„ ê¸°ê°„", f"{len(aggregated_df)}ê°œ êµ¬ê°„")
                with col3:
                    st.metric("ë¶„ì„ ì§€í‘œ", len(y_cols))
                
                fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                st.plotly_chart(fig, use_container_width=True)
                
                if show_table:
                    st.subheader("ì§‘ê³„ëœ ë°ì´í„°")
                    st.dataframe(aggregated_df, use_container_width=True)
                    
                    csv = aggregated_df.to_csv(index=False)
                    st.download_button(
                        label="ì§‘ê³„ëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                        data=csv,
                        file_name=f"aggregated_data_{agg_method}.csv",
                        mime="text/csv"
                    )
        
    else:  # ìˆ˜ë™ ì„ íƒ ëª¨ë“œ
        st.subheader("ìˆ˜ë™ ì»¬ëŸ¼ ì„ íƒ")
        
        available_cols = column_analysis['all_columns']
        
        default_index = column_analysis['all_columns'].index('timestamp') if 'timestamp' in column_analysis['all_columns'] else 0
        x_col = st.selectbox("Xì¶• ì»¬ëŸ¼", available_cols, index=default_index)
        y_cols = st.multiselect("Yì¶• ì»¬ëŸ¼(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", available_cols)
        
        if x_col and y_cols and st.button("íŒ¨í„´ ë¶„ì„ ì‹¤í–‰"):
            with st.spinner("ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                aggregated_df = create_aggregated_dataframe(csv_files, [x_col], y_cols, agg_method)
            
            if aggregated_df is not None:
                fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                st.plotly_chart(fig, use_container_width=True)
                
                if show_table:
                    st.dataframe(aggregated_df, use_container_width=True)
else:
    if data_source == "í´ë” ë¶„ì„":
        st.warning("âš ï¸ ì§€ì •ëœ í´ë”ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

