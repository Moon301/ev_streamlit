from calendar import c
from ntpath import samefile
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import os
import numpy as np
from pathlib import Path
import re
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(page_title="ë°ì´í„° ì‹œê°í™” ëŒ€ì‹œë³´ë“œ", layout="wide")

if 'draw_graph' not in st.session_state:
    st.session_state['draw_graph'] = False

def load_csv_files_from_folder(folder_path):
    """í´ë”ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ë¡œë“œ"""
    csv_files = []
    folder = Path(folder_path)
    
    if not folder.exists():
        return []
    
    for file_path in folder.glob("*.csv"):
        try:
            df = pd.read_csv(file_path)
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬: ì•ë’¤ ê³µë°± ì œê±°
            df.columns = df.columns.str.strip()
            
            # ë¹ˆ ì»¬ëŸ¼ëª…ì´ë‚˜ ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
            df.columns = [col if col else f'unnamed_{i}' for i, col in enumerate(df.columns)]
            
            # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
            cols = df.columns.tolist()
            seen = {}
            for i, col in enumerate(cols):
                if col in seen:
                    seen[col] += 1
                    cols[i] = f"{col}_{seen[col]}"
                else:
                    seen[col] = 0
            df.columns = cols
            
            csv_files.append({
                'filename': file_path.name,
                'filepath': str(file_path),
                'dataframe': df,
                'shape': df.shape
            })
        except Exception as e:
            st.warning(f"{file_path.name} íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    
    return csv_files

def analyze_column_relationships(dataframes):
    """ì»¬ëŸ¼ë“¤ ê°„ì˜ ê´€ê³„ì„± ë¶„ì„"""
    all_columns = set()
    column_types = {}
    
    # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ì—ì„œ ê³µí†µ ì»¬ëŸ¼ ì°¾ê¸°
    for df_info in dataframes:
        df = df_info['dataframe']
        all_columns.update(df.columns)
        
        for col in df.columns:
            if col not in column_types:
                column_types[col] = []
            column_types[col].append(df[col].dtype)
    
    # ì»¬ëŸ¼ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    timestamp_cols = []
    numeric_cols = []
    categorical_cols = []
    
    for col in all_columns:
        # ì»¬ëŸ¼ëª… ì •ë¦¬ í›„ ë¶„ì„
        col_clean = str(col).strip().lower()
        
        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì‹ë³„
        if any(keyword in col_clean for keyword in ['timestamp', 'time', 'date',  'ì‹œê°„', 'ë‚ ì§œ']):
            timestamp_cols.append(col)
        # ìˆ«ìí˜• ì»¬ëŸ¼ ì‹ë³„
        elif any(pd.api.types.is_numeric_dtype(dtype) for dtype in column_types[col]):
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)
    
    return {
        'timestamp_cols': timestamp_cols,
        'numeric_cols': numeric_cols,
        'categorical_cols': categorical_cols,
        'all_columns': list(all_columns)
    }

def group_columns_by_keywords(columns, keywords):
    """í‚¤ì›Œë“œë³„ë¡œ ì»¬ëŸ¼ë“¤ì„ ê·¸ë£¹í™”"""
    groups = {}
    
    for keyword in keywords:
        keyword_lower = keyword.lower().strip()
        matching_cols = []
        
        for col in columns:
            # ì»¬ëŸ¼ëª…ë„ ì•ë’¤ ê³µë°± ì œê±° í›„ ë¹„êµ
            col_clean = str(col).strip().lower()
            if keyword_lower in col_clean:
                matching_cols.append(col)
        
        if matching_cols:
            groups[keyword] = matching_cols
    
    return groups

def create_keyword_aggregated_dataframe(dataframes, x_col, keyword_groups, agg_method='mean', time_agg_method='ì •í™•í•œ ì‹œê°„'):
    """í‚¤ì›Œë“œ ê·¸ë£¹ë³„ë¡œ ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    combined_data = []
    
    for df_info in dataframes:
        df = df_info['dataframe'].copy()
        df['source_file'] = df_info['filename']
        
        # Xì¶• ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ê³µë°± ì œê±° í›„)
        x_col_clean = str(x_col).strip()
        if x_col_clean not in df.columns:
            continue
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ë¥¼ ë¨¼ì € ìˆ˜í–‰
        col_clean = str(x_col).strip().lower()
        if any(keyword in col_clean for keyword in ['time', 'date', 'timestamp']):
            try:
                df[x_col_clean] = pd.to_datetime(df[x_col_clean])
                
                # ì‹œê°„ ì§‘ê³„ ë°©ë²•ì— ë”°ë¼ ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜
                if time_agg_method == "ì‹œê°„ë³„":
                    df['time_group'] = df[x_col_clean].dt.floor('H')  # ì‹œê°„ë³„ë¡œ ê·¸ë£¹í™”
                elif time_agg_method == "ì¼ë³„":
                    df['time_group'] = df[x_col_clean].dt.date  # ì¼ë³„ë¡œ ê·¸ë£¹í™”
                elif time_agg_method == "ì›”ë³„":
                    df['time_group'] = df[x_col_clean].dt.to_period('M')  # ì›”ë³„ë¡œ ê·¸ë£¹í™”
                else:  # ì •í™•í•œ ì‹œê°„
                    df['time_group'] = df[x_col_clean]
                    
            except Exception as e:
                st.warning(f"ì‹œê°„ ì»¬ëŸ¼ ë³€í™˜ ì‹¤íŒ¨: {e}")
                df['time_group'] = df[x_col_clean]
        else:
            df['time_group'] = df[x_col_clean]
            
        df_subset = df[['time_group', 'source_file']].copy()
        
        # ê° í‚¤ì›Œë“œ ê·¸ë£¹ë³„ë¡œ í‰ê·  ê³„ì‚°
        for keyword, cols in keyword_groups.items():
            # ì»¬ëŸ¼ëª… ì •ë¦¬ í›„ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ ì°¾ê¸°
            available_cols = []
            for col in cols:
                col_clean = str(col).strip()
                if col_clean in df.columns and pd.api.types.is_numeric_dtype(df[col_clean]):
                    available_cols.append(col_clean)
            
            if available_cols:
                try:
                    if agg_method == 'mean':
                        df_subset[f'{keyword}_í‰ê· '] = df[available_cols].mean(axis=1)
                    elif agg_method == 'sum':
                        df_subset[f'{keyword}_í•©ê³„'] = df[available_cols].sum(axis=1)
                    elif agg_method == 'median':
                        df_subset[f'{keyword}_ì¤‘ì•™ê°’'] = df[available_cols].median(axis=1)
                    
                    # ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
                    st.write(f"âœ… '{keyword}' ê·¸ë£¹: {len(available_cols)}ê°œ ì»¬ëŸ¼ ì§‘ê³„ë¨")
                    with st.expander(f"'{keyword}' ê·¸ë£¹ ìƒì„¸"):
                        st.write(f"ì‚¬ìš©ëœ ì»¬ëŸ¼: {available_cols}")
                        
                except Exception as e:
                    st.warning(f"âš ï¸ '{keyword}' ê·¸ë£¹ ì§‘ê³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        combined_data.append(df_subset)
    
    if not combined_data:
        return None
    
    # ë°ì´í„° í†µí•©
    combined_df = pd.concat(combined_data, ignore_index=True)
    
    # time_group ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„
    value_cols = [col for col in combined_df.columns if col not in ['time_group', 'source_file']]
    
    if not value_cols:
        st.error("ì§‘ê³„í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        if agg_method == 'mean':
            agg_df = combined_df.groupby('time_group')[value_cols].mean().reset_index()
        elif agg_method == 'sum':
            agg_df = combined_df.groupby('time_group')[value_cols].sum().reset_index()
        elif agg_method == 'median':
            agg_df = combined_df.groupby('time_group')[value_cols].median().reset_index()
        else:
            agg_df = combined_df.groupby('time_group')[value_cols].mean().reset_index()
        
        # ì›ë˜ ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€ê²½
        agg_df = agg_df.rename(columns={'time_group': x_col_clean})
        
        # ì§‘ê³„ ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        st.info(f"ğŸ“Š ì§‘ê³„ ì™„ë£Œ: {len(agg_df)}ê°œì˜ ì‹œê°„ êµ¬ê°„, {len(value_cols)}ê°œì˜ í‚¤ì›Œë“œ ê·¸ë£¹")
        
        # ì‹œê°„ ë²”ìœ„ í‘œì‹œ
        if len(agg_df) > 0:
            time_range_start = agg_df[x_col_clean].min()
            time_range_end = agg_df[x_col_clean].max()
            st.write(f"â° ë¶„ì„ ê¸°ê°„: {time_range_start} ~ {time_range_end}")
        
        return agg_df
    except Exception as e:
        st.error(f"ë°ì´í„° ì§‘ê³„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.write(f"ë””ë²„ê·¸ ì •ë³´: ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ = {value_cols}")
        return None

def create_aggregated_dataframe(dataframes, group_cols, value_cols, agg_method='mean'):
    """ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆì„ì„ í†µí•©í•˜ì—¬ ì§‘ê³„ëœ ë°ì´í„°í”„ë ˆì„ ìƒì„±"""
    combined_data = []
    
    for df_info in dataframes:
        df = df_info['dataframe'].copy()
        df['source_file'] = df_info['filename']
        
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_cols = [col for col in group_cols + value_cols if col in df.columns]
        if len(available_cols) < 2:
            continue
            
        df_subset = df[available_cols + ['source_file']]
        combined_data.append(df_subset)
    
    if not combined_data:
        return None
    
    # ë°ì´í„° í†µí•©
    combined_df = pd.concat(combined_data, ignore_index=True)
    
    # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
    for col in group_cols:
        if col in combined_df.columns:
            col_lower = col.lower().strip()
            if any(keyword in col_lower for keyword in ['timestamp', 'time', 'date', ]):
                try:
                    combined_df[col] = pd.to_datetime(combined_df[col])
                except:
                    pass
    
    # ì§‘ê³„ ìˆ˜í–‰
    if agg_method == 'mean':
        agg_df = combined_df.groupby(group_cols)[value_cols].mean().reset_index()
    elif agg_method == 'sum':
        agg_df = combined_df.groupby(group_cols)[value_cols].sum().reset_index()
    elif agg_method == 'median':
        agg_df = combined_df.groupby(group_cols)[value_cols].median().reset_index()
    else:
        agg_df = combined_df.groupby(group_cols)[value_cols].mean().reset_index()
    
    return agg_df

def create_pattern_analysis_chart(df, x_col, y_cols, chart_type="Line"):
    """íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ ì°¨íŠ¸ ìƒì„±"""
    if chart_type == "Multi-Line with Correlation":
        # ì„œë¸Œí”Œë¡¯ ìƒì„± (ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ í¬í•¨)
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ì‹œê³„ì—´ íŒ¨í„´', 'ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ', 'ë¶„í¬ ë¶„ì„', 'íŠ¸ë Œë“œ ë¶„ì„'),
            specs=[[{"colspan": 2}, None],
                   [{"type": "xy"}, {"type": "xy"}]]
        )
        
        # 1. ì‹œê³„ì—´ íŒ¨í„´
        for y_col in y_cols:
            if y_col in df.columns:
                fig.add_trace(
                    go.Scatter(x=df[x_col], y=df[y_col], name=y_col, mode='lines+markers'),
                    row=1, col=1
                )
        
        # 2. ìƒê´€ê´€ê³„ ë¶„ì„ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ)
        numeric_cols = [col for col in y_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            
            fig.add_trace(
                go.Heatmap(
                    z=corr_matrix.values,
                    x=corr_matrix.columns,
                    y=corr_matrix.columns,
                    colorscale='RdBu',
                    zmid=0
                ),
                row=2, col=1
            )
        
        # 3. ë¶„í¬ ë¶„ì„ (ë°•ìŠ¤í”Œë¡¯)
        for i, y_col in enumerate(numeric_cols[:3]):  # ìµœëŒ€ 3ê°œë§Œ
            fig.add_trace(
                go.Box(y=df[y_col], name=y_col),
                row=2, col=2
            )
        
        fig.update_layout(height=800, title_text="ì¢…í•© íŒ¨í„´ ë¶„ì„")
        return fig
    
    elif chart_type == "Normalized Comparison":
        # ì •ê·œí™”ëœ ë¹„êµ ì°¨íŠ¸
        fig = go.Figure()
        
        # ê° ì»¬ëŸ¼ì„ 0-1 ë²”ìœ„ë¡œ ì •ê·œí™”
        for y_col in y_cols:
            if y_col in df.columns and pd.api.types.is_numeric_dtype(df[y_col]):
                normalized_values = (df[y_col] - df[y_col].min()) / (df[y_col].max() - df[y_col].min())
                fig.add_trace(
                    go.Scatter(
                        x=df[x_col], 
                        y=normalized_values, 
                        name=f"{y_col} (ì •ê·œí™”)", 
                        mode='lines+markers'
                    )
                )
        
        fig.update_layout(
            title="ì •ê·œí™”ëœ ë°ì´í„° ë¹„êµ (0-1 ìŠ¤ì¼€ì¼)",
            yaxis_title="ì •ê·œí™”ëœ ê°’",
            height=600
        )
        return fig
    
    else:
        # ê¸°ë³¸ ì°¨íŠ¸ë“¤
        if chart_type == "Line":
            fig = px.line(df, x=x_col, y=y_cols, title="ë¼ì¸ ì°¨íŠ¸ - íŒ¨í„´ ë¶„ì„")
        elif chart_type == "Scatter":
            fig = px.scatter(df, x=x_col, y=y_cols, title="ì‚°ì ë„ - íŒ¨í„´ ë¶„ì„")
        elif chart_type == "Bar":
            fig = px.bar(df, x=x_col, y=y_cols, title="ë§‰ëŒ€ ì°¨íŠ¸ - íŒ¨í„´ ë¶„ì„")
        
        return fig

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.subheader("Server Info")
    st.markdown("**ì‹œí¥ gpuserver2** (59.14.241.229) - 5090*3")
        
    st.title("âš™ï¸ ì„¤ì •")   

    # ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
    data_source = st.radio("ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ", ["ë‹¨ì¼ íŒŒì¼", "í´ë” ë¶„ì„"])
    
    if data_source == "ë‹¨ì¼ íŒŒì¼":
        uploaded_file = st.file_uploader("CSV íŒŒì¼ ì—…ë¡œë“œ", type=["csv"])
        use_sample_data = st.checkbox("ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")
    else:
        folder_path = st.text_input("í´ë” ê²½ë¡œ ì…ë ¥", value="./sample")
        
    # ë¶„ì„ ëª¨ë“œ ì„ íƒ
    analysis_mode = st.radio("ë¶„ì„ ëª¨ë“œ", ["í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„", "ìˆ˜ë™ ì„ íƒ"]) # "ìë™ íŒ¨í„´ ë¶„ì„" ì¼ë‹¨ ì œê±°í•¨
        
    if analysis_mode == "í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„":
        st.subheader("ğŸ” í‚¤ì›Œë“œ ì„¤ì •")
        
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì œì•ˆ
        default_keywords = st.text_input(
            "ë¶„ì„í•  ê¸°ë³¸ í‚¤ì›Œë“œ(ì‰¼í‘œë¡œ êµ¬ë¶„)", 
            value="cell, temperature, current, soc"
        )
        keywords = [k.strip() for k in default_keywords.split(",") if k.strip()]
        
        # ì¶”ê°€ í‚¤ì›Œë“œ
        additional_keywords = st.text_area(
            "ì¶”ê°€ í‚¤ì›Œë“œ (í•œ ì¤„ì— í•˜ë‚˜ì”©)",
            placeholder="ì˜ˆ:\speed\mileage\soh"
        )
        if additional_keywords:
            keywords.extend([k.strip() for k in additional_keywords.split("\n") if k.strip()])
    
    agg_method = st.selectbox("ì§‘ê³„ ë°©ë²•", ["mean", "sum", "median"])
    
    chart_type = st.selectbox("ì°¨íŠ¸ íƒ€ì… ì„ íƒ", [
        "Line", "Scatter", "Bar", 
        "Multi-Line with Correlation", 
        "Normalized Comparison"
    ])
    show_table = st.checkbox("ì „ì²´ ë°ì´í„° í…Œì´ë¸” ë³´ê¸°")

# ë©”ì¸ í™”ë©´
st.title("ğŸ“Š ë°ì´í„° ì‹œê°í™” ëŒ€ì‹œë³´ë“œ")
st.write("ì—…ë¡œë“œí•œ ë°ì´í„° ë˜ëŠ” í´ë” ë‚´ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ì°¨íŠ¸ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# í´ë” ë¶„ì„ ëª¨ë“œ
if data_source == "í´ë” ë¶„ì„":
    if folder_path:
        csv_files = load_csv_files_from_folder(folder_path)
else:
    csv_files = []
    df = pd.DataFrame()

    try:
        # ë°ì´í„° ì½ê¸°
        if use_sample_data:
            filepath = "sample/628dani_V031BL0000_CASPER LONGRANGE_202410.csv"
            df = pd.read_csv(filepath)
            filename = "sampledata.csv"
        elif uploaded_file:
            df = pd.read_csv(uploaded_file)
            filepath = str(uploaded_file)
            filename = uploaded_file.name
        else:
            raise ValueError("íŒŒì¼ ì—†ìŒ")

        # ì»¬ëŸ¼ëª… ì •ë¦¬
        df.columns = df.columns.str.strip()  # ì•ë’¤ ê³µë°± ì œê±°
        df.columns = [col if col else f'unnamed_{i}' for i, col in enumerate(df.columns)]  # ë¹ˆ ì»¬ëŸ¼ ì²˜ë¦¬

        # ì¤‘ë³µ ì»¬ëŸ¼ëª… ì²˜ë¦¬
        seen = {}
        new_cols = []
        for col in df.columns:
            if col in seen:
                seen[col] += 1
                new_cols.append(f"{col}_{seen[col]}")
            else:
                seen[col] = 0
                new_cols.append(col)
        df.columns = new_cols

        # ê²°ê³¼ ì¶”ê°€
        csv_files.append({
            'filename': filename,
            'filepath': filepath,
            'dataframe': df,
            'shape': df.shape
        })

    except Exception as e:
        st.warning("âš ï¸ CSVë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        
              

        
if csv_files:
    st.success(f"âœ… {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ ëª©ë¡ í‘œì‹œ
    with st.expander("ë°œê²¬ëœ íŒŒì¼ë“¤"):
        for file_info in csv_files:
            st.write(f"- **{file_info['filename']}**: {file_info['shape']} (í–‰, ì—´)")
    
    # ì»¬ëŸ¼ ê´€ê³„ ë¶„ì„
    column_analysis = analyze_column_relationships(csv_files)
    
    st.subheader("ğŸ” ì»¬ëŸ¼ ë¶„ì„ ê²°ê³¼")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.write("**ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼**")
        for col in column_analysis['timestamp_cols']:
            st.write(f"- {col}")
    
    with col2:
        st.write("**ìˆ«ìí˜• ì»¬ëŸ¼**")
        for col in column_analysis['numeric_cols'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            st.write(f"- {col}")
        if len(column_analysis['numeric_cols']) > 10:
            st.write(f"... ì™¸ {len(column_analysis['numeric_cols']) - 10}ê°œ")
    
    with col3:
        st.write("**ë²”ì£¼í˜• ì»¬ëŸ¼**")
        for col in column_analysis['categorical_cols'][:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            st.write(f"- {col}")
        if len(column_analysis['categorical_cols']) > 10:
            st.write(f"... ì™¸ {len(column_analysis['categorical_cols']) - 10}ê°œ")
    
    # ë¶„ì„ ëª¨ë“œë³„ ì²˜ë¦¬
    if analysis_mode == "í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„":
        # í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„
        if keywords and column_analysis['timestamp_cols']:
            
            st.divider()
            st.subheader("ğŸ•’ ì‹œê°„ ì§‘ê³„ ì„¤ì •")
            
            default_index = column_analysis['timestamp_cols'].index('timestamp') if 'timestamp' in column_analysis['timestamp_cols'] else 0
            
            x_col = st.selectbox("ì‹œê°„ì¶• ì»¬ëŸ¼", column_analysis['timestamp_cols'], index=default_index)
            
            # ì‹œê°„ ì§‘ê³„ ë°©ë²• ì„ íƒ ì¶”ê°€
            time_agg_method = st.selectbox(
                "ì‹œê°„ ì§‘ê³„ ë°©ë²•", 
                ["ì •í™•í•œ ì‹œê°„", "ì‹œê°„ë³„", "ì¼ë³„", "ì›”ë³„"],
                help="ì„œë¡œ ë‹¤ë¥¸ ì‹œê°„ëŒ€ì˜ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì§‘ê³„í• ì§€ ì„ íƒí•˜ì„¸ìš”"
            )
            
            if time_agg_method != "ì •í™•í•œ ì‹œê°„":
                st.info(f"ğŸ’¡ {time_agg_method} ì§‘ê³„: ê°™ì€ {time_agg_method.replace('ë³„', '')} ë‚´ì˜ ë°ì´í„°ë“¤ì„ í‰ê· í™”í•©ë‹ˆë‹¤")
            
            # í‚¤ì›Œë“œë³„ ì»¬ëŸ¼ ê·¸ë£¹í™”
            keyword_groups = group_columns_by_keywords(column_analysis['numeric_cols'], keywords)
            
            st.divider()
            if keyword_groups:
                st.subheader("ğŸ“Š í‚¤ì›Œë“œë³„ ì»¬ëŸ¼ ê·¸ë£¹")
                
                # ê° í‚¤ì›Œë“œ ê·¸ë£¹ í‘œì‹œ
                for keyword, cols in keyword_groups.items():
                    with st.expander(f"ğŸ”¹ '{keyword}' ê´€ë ¨ ì»¬ëŸ¼ë“¤ ({len(cols)}ê°œ)"):
                        for col in cols:
                            st.write(f"- {col}")
                
                # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                if st.button("ğŸš€ í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„ ì‹¤í–‰", type="primary"):
                    with st.spinner("í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        aggregated_df = create_keyword_aggregated_dataframe(
                            csv_files, x_col, keyword_groups, agg_method, time_agg_method
                        )
                    
                    if aggregated_df is not None and len(aggregated_df) > 0:
                        st.subheader("ğŸ“ˆ í‚¤ì›Œë“œ ê·¸ë£¹ë³„ íŒ¨í„´ ë¶„ì„")
                        
                        # ìš”ì•½ í†µê³„
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("ë¶„ì„ëœ í‚¤ì›Œë“œ ê·¸ë£¹", len(keyword_groups))
                        with col2:
                            st.metric("ë°ì´í„° í¬ì¸íŠ¸", len(aggregated_df))
                        with col3:
                            group_cols = [col for col in aggregated_df.columns if col != x_col]
                            st.metric("ìƒì„±ëœ ì§€í‘œ", len(group_cols))
                        
                        # í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì°¨íŠ¸
                        y_cols = [col for col in aggregated_df.columns if col != x_col]
                        
                        if chart_type == "Multi-Line with Correlation":
                            # ì„œë¸Œí”Œë¡¯ ìƒì„±
                            fig = make_subplots(
                                rows=2, cols=2,
                                subplot_titles=('í‚¤ì›Œë“œ ê·¸ë£¹ë³„ íŠ¸ë Œë“œ', 'ê·¸ë£¹ê°„ ìƒê´€ê´€ê³„', 'ë¶„í¬ ë¹„êµ', 'ì •ê·œí™” ë¹„êµ'),
                                specs=[[{"colspan": 2}, None],
                                        [{"type": "xy"}, {"type": "xy"}]]
                            )
                            
                            # 1. í‚¤ì›Œë“œ ê·¸ë£¹ë³„ íŠ¸ë Œë“œ
                            for y_col in y_cols:
                                fig.add_trace(
                                    go.Scatter(x=aggregated_df[x_col], y=aggregated_df[y_col], 
                                                name=y_col, mode='lines+markers'),
                                    row=1, col=1
                                )
                            
                            # 2. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
                            if len(y_cols) > 1:
                                corr_matrix = aggregated_df[y_cols].corr()
                                fig.add_trace(
                                    go.Heatmap(
                                        z=corr_matrix.values,
                                        x=corr_matrix.columns,
                                        y=corr_matrix.columns,
                                        colorscale='RdBu',
                                        zmid=0,
                                        showscale=True
                                    ),
                                    row=2, col=1
                                )
                            
                            # 3. ì •ê·œí™” ë¹„êµ
                            for y_col in y_cols:
                                if pd.api.types.is_numeric_dtype(aggregated_df[y_col]):
                                    col_data = aggregated_df[y_col]
                                    normalized = (col_data - col_data.min()) / (col_data.max() - col_data.min())
                                    fig.add_trace(
                                        go.Scatter(x=aggregated_df[x_col], y=normalized, 
                                                    name=f"{y_col} (ì •ê·œí™”)", mode='lines'),
                                        row=2, col=2
                                    )
                            
                            fig.update_layout(height=800, title_text="í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ì¢…í•© ë¶„ì„")
                            st.plotly_chart(fig, use_container_width=True)
                        
                        else:
                            # ê¸°ë³¸ ì°¨íŠ¸
                            fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ìƒì„¸ í†µê³„ ì •ë³´
                        with st.expander("ğŸ“‹ ìƒì„¸ í†µê³„ ì •ë³´"):
                            st.write("**ê° í‚¤ì›Œë“œ ê·¸ë£¹ë³„ ê¸°ë³¸ í†µê³„:**")
                            for col in y_cols:
                                col_stats = aggregated_df[col].describe()
                                st.write(f"**{col}**")
                                st.write(f"- í‰ê· : {col_stats['mean']:.2f}")
                                st.write(f"- í‘œì¤€í¸ì°¨: {col_stats['std']:.2f}")
                                st.write(f"- ìµœì†Ÿê°’: {col_stats['min']:.2f}")
                                st.write(f"- ìµœëŒ“ê°’: {col_stats['max']:.2f}")
                                st.write("---")
                        
                        # ë°ì´í„° í…Œì´ë¸” (ì˜µì…˜)
                        if show_table:
                            st.subheader("ì§‘ê³„ëœ í‚¤ì›Œë“œ ê·¸ë£¹ ë°ì´í„°")
                            st.dataframe(aggregated_df, use_container_width=True)
                            
                            # CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            csv = aggregated_df.to_csv(index=False)
                            st.download_button(
                                label="í‚¤ì›Œë“œ ê·¸ë£¹ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (CSV)",
                                data=csv,
                                file_name=f"keyword_group_analysis_{agg_method}.csv",
                                mime="text/csv"
                            )
                    else:
                        st.warning("âš ï¸ ë¶„ì„í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë‚˜ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            else:
                st.warning("âš ï¸ ì§€ì •ëœ í‚¤ì›Œë“œì™€ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ’¡ ë¶„ì„í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  ì‹œê°„ì¶• ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    elif analysis_mode == "ìë™ íŒ¨í„´ ë¶„ì„":
        # ê¸°ì¡´ ìë™ ë¶„ì„ ë¡œì§
        if column_analysis['timestamp_cols'] and column_analysis['numeric_cols']:
            
            default_index = column_analysis['timestamp_cols'].index('timestamp') if 'timestamp' in column_analysis['timestamp_cols'] else 0
            
            x_col = column_analysis['timestamp_cols'][default_index]
            y_cols = column_analysis['numeric_cols'][:5]
            
            st.success(f"ğŸ¤– ìë™ ì„ íƒ: Xì¶•={x_col}, Yì¶•={', '.join(y_cols)}")
            
            with st.spinner("ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                aggregated_df = create_aggregated_dataframe(csv_files, [x_col], y_cols, agg_method)
            
            if aggregated_df is not None:
                st.subheader("ğŸ“ˆ íŒ¨í„´ ë¶„ì„ ê²°ê³¼")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì´ ë°ì´í„° í¬ì¸íŠ¸", len(aggregated_df))
                with col2:
                    st.metric("ë¶„ì„ ê¸°ê°„", f"{len(aggregated_df)}ê°œ êµ¬ê°„")
                with col3:
                    st.metric("ë¶„ì„ ì§€í‘œ", len(y_cols))
                
                fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                st.plotly_chart(fig, use_container_width=True)
                
                if show_table:
                    st.subheader("ì§‘ê³„ëœ ë°ì´í„°")
                    st.dataframe(aggregated_df, use_container_width=True)
                    
                    csv = aggregated_df.to_csv(index=False)
                    st.download_button(
                        label="ì§‘ê³„ëœ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                        data=csv,
                        file_name=f"aggregated_data_{agg_method}.csv",
                        mime="text/csv"
                    )
        
    else:  # ìˆ˜ë™ ì„ íƒ ëª¨ë“œ
        st.subheader("ìˆ˜ë™ ì»¬ëŸ¼ ì„ íƒ")
        
        available_cols = column_analysis['all_columns']
        x_col = st.selectbox("Xì¶• ì»¬ëŸ¼", available_cols)
        y_cols = st.multiselect("Yì¶• ì»¬ëŸ¼(ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)", available_cols)
        
        if x_col and y_cols and st.button("íŒ¨í„´ ë¶„ì„ ì‹¤í–‰"):
            with st.spinner("ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                aggregated_df = create_aggregated_dataframe(csv_files, [x_col], y_cols, agg_method)
            
            if aggregated_df is not None:
                fig = create_pattern_analysis_chart(aggregated_df, x_col, y_cols, chart_type)
                st.plotly_chart(fig, use_container_width=True)
                
                if show_table:
                    st.dataframe(aggregated_df, use_container_width=True)
else:
    if data_source == "í´ë” ë¶„ì„":
        st.warning("âš ï¸ ì§€ì •ëœ í´ë”ì—ì„œ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

